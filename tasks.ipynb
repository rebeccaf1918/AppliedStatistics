{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Required Modules\n",
    "\n",
    "## Mathematical functions from the standard library\n",
    "# https://docs.python.org/3/library/math.html  \n",
    "import math \n",
    "\n",
    "## Numerical structures and operations\n",
    "# https://numpy.org/doc/stable/reference/index.html  \n",
    "import numpy as np \n",
    "\n",
    "## Plotting\n",
    "# https://matplotlib.org/stable/contents.html  \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Random selections\n",
    "# https://docs.python.org/3/library/random.html  \n",
    "import random\n",
    "\n",
    "## Permutations and combinations\n",
    "# https://docs.python.org/3/library/itertools.html  \n",
    "import itertools\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "### Statistics\n",
    "import scipy.stats as stats  \n",
    "import seaborn as sns \n",
    "import statistics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "\n",
    "Task 1 - Permutations and Combinations  \n",
    "Completed by: Rebecca Feeley  \n",
    "\n",
    "Suppose we alter the Lady Tasting Tea experiment to involve twelve cups of tea. Six have the milk in first and the other six having tea in first. A person claims they have the special power of being able to tell whether the tea or the milk went into a cup first upon tasting it. You agree to accept their claim if they can tell which of the six cups in your experiment had the milk in first.\n",
    "\n",
    "Calculate, using Python, the probability that they select the correct six cups. Here you should assume that they have no special powers in figuring it out, that they are just guessing. Remember to show and justify your workings in code and MarkDown cells.\n",
    "\n",
    "Suppose, now, you are willing to accept one error. Once they select the six cups they think had the milk in first, you will give them the benefit of the doubt should they have selected at least five of the correct cups. Calculate the probability, assuming they have no special powers, that the person makes at most one error.\n",
    "\n",
    "Would you accept two errors? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1  Explanation and Planned Solution\n",
    "This task is a proposed expansion of the famous Lady Tasting Tea test to involve 12 cups of tea, 6 of which have the milk added in first and 6 of which have the tea added in first.\n",
    "\n",
    "This experiment, conducted and published by Ronald Fisher in 1935, set out to determine how the cups of tea should be prepared (i.e should they be randomised) in order to establish that the lady had a genuine ability to tell how the cups of tea were prepared rather than lucky guesses on the part of the lady.   \n",
    "A key aspect of this experiment is the idea of the null hypothesis - that is an initial statement, as close to the real world application as possible, which is treated as true until it is proven otherwise. Fisher's null hypothesis was that the lady testing the tea did not have any unique ability to determine how the tea was prepared and that her claim to be able to do so was based on chance (i.e 'lucky guesses').    \n",
    "Fisher firstly set out to determine the probability of idenifying the correct preparation of each cup based on chance alone. He determined this to be approximately 1 in 70. As the chance of the lady correctly idenitifying each cup was so low, when the lady correctly determined the preparation process of each cup, Fisher was then able to reject the null hypothesis and concluded that \"her ability to discern the tea preparation was statistically significant.\"\n",
    "However, as the lady was able to correctly determine the preparation process of each cup, Fisher rejected the null hypothesis.   \n",
    "\n",
    "\n",
    "The first part of this task invloves calculating the probability that the person guesses each of the cups by chance, as Fisher set out to determine. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of cups of tea in total.\n",
    "no_cups_total = 12\n",
    "\n",
    "# Number of cups of tea with milk added first.\n",
    "no_cups_milk_first = 6\n",
    "\n",
    "# Number of cups of tea with tea added first.\n",
    "no_cups_tea_first = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have imported the math module to use the comb function to calculate the number of ways to choose k elements from a set of n elements where the order of the chosen elements does not matter and no element is selected more than once.\n",
    "In this case, k elements refers to number of cups with milk added first and n elements is the total amount of cups.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of ways of selecting six cups from twelve using comb function\n",
    "ways = math.comb(no_cups_total, no_cups_milk_first)\n",
    "\n",
    "print (\"The number of ways of selecting six cups from twelve is:\", ways)\n",
    "\n",
    "# This function could also be used to select six cups from twelve (i.e the six cups with the tea added first)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have established how many ways of choosing six cups from twelve exist, we will look at the probability that a person tasting the tea would choose the correct six cups purely by chance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the probability of choosing the correct six cups, we divide 1 by the number of ways \n",
    "# of selecting 6 separate cups from twelve\n",
    "# We divide into 1 as this is considered the correct conbination, and we divide into this by the total number of combinations\n",
    "# in order to get the probability of choosing the correct cups.\n",
    "\n",
    "correct_guess = 1\n",
    "probability_all_correct = correct_guess/ways \n",
    "print(f\"Probability of selecting the correct 6 cups: {probability_all_correct:.10f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to show the to demonstrate the different ways of selecting the cups with milk first out of the cups of tea (i.e selecting six cups out of 12 cups) is to give each cup a label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning a label to each cup\n",
    "labels = list(range(no_cups_total))\n",
    "\n",
    "print(\"Each cup is labelled:\", labels)\n",
    "\n",
    "# Show the different ways of selecting cups with milk first out of the cups of tea.\n",
    "combs = list(itertools.combinations(labels, no_cups_milk_first))\n",
    "\n",
    "# 'combs' is commented out for clarity as it gives a long list of each of the ways of selecting 6 out of 12 cups\n",
    "# combs\n",
    "\n",
    "# Number of combinations of choosing 6 cups out of 12\n",
    "len(combs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is demonstrated above, there are 924 different ways of choosing 6 cups out of 12 cups. \n",
    "\n",
    "In order to determine the likelihood of the person making at most one error, I will simulate the guessing process of the lady by randomly assigning 6 cups out of the 12 to have the milk added first. \n",
    "Then, for each guessed set of six cups, I will determine how many of these cups match with the actual set of 6 cups chosen to have the milk added first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select six cups at random which have the milk added first (this changes each time the program is ran to ensure randomness)\n",
    "labels_milk = random.sample(labels, 6)\n",
    "\n",
    "# Putting the labels in order using the sort() function\n",
    "labels_milk.sort()\n",
    "print(\"The cups with milk added first are labelled:\", labels_milk)\n",
    "\n",
    "\n",
    "# creating a list to store the number of overlaps\n",
    "no_overlaps = []\n",
    "\n",
    "# using the for function to iterate through each combination in comb\n",
    "for comb in combs:\n",
    "    # Turn comb and labels_milk into sets.\n",
    "    s1 = set(comb)\n",
    "    s2 = set(labels_milk)\n",
    "    # Determine where they overlap i.e the number of correct guesses in each combination\n",
    "    overlap = s1.intersection(s2)\n",
    "    # Adding the overlap to no_overlaps list\n",
    "    no_overlaps.append(len(overlap))\n",
    "\n",
    "# Count the number of times each overlap occurs\n",
    "unique, counts = np.unique(no_overlaps, return_counts=True)\n",
    "\n",
    "print(\"Number of correct guesses for each label : Frequency\")\n",
    "for u, c in zip(unique, counts):\n",
    "    print(f\"{u} : {c}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of the overlap code demonstrate the likelihood of how many correct cups a person will identify if guessing blindly. A person incorrectly identifying all cups with milk added first and correctly identifying all cups with milk added first are the most unlikely outcomes. \n",
    "A person determining 3 out of the 6 cups with milk added correctly is the most likely outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising the results of the above code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.bar(unique, counts)\n",
    "ax.set_xlabel('Number of Correct Guesses')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Distribution of Correct Guesses in Lady Tasting Tea Experiment (12 cups of tea involved)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will look at the probability of getting at most one error. The above code and bar chart shows the likelihood of getting 5 cups correct is 36. Thus, in order to determine the probability, of at most one error, this means the person could also make no error. \n",
    "So, it is the frequency of choosing 5 cups correct (36) and the frequency of choosing all cups correctly (1), divided by the total number of ways of choosing 6 cups. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequencies of getting 5 and 6 cups correct\n",
    "count_5_correct = 36\n",
    "count_6_correct = 1\n",
    "\n",
    "# Probability of making at most one error\n",
    "probability_one_or_less_errors = (count_5_correct + count_6_correct) / ways\n",
    "print(f\"Probability of making at most one error: {probability_one_or_less_errors:.10f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem also asks if I could accept 2 errors, so I am going to calculate the person making two errors when choosing the cups with milk added first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequencies of getting 5 and 6 cups correct\n",
    "count_4_correct = 225\n",
    "\n",
    "\n",
    "# Probability of making at most one error\n",
    "probability_one_or_less_errors = (count_4_correct) / ways\n",
    "print(f\"Probability of making two errors: {probability_one_or_less_errors:.10f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of the probabilities\n",
    "\n",
    "Firstly, looking at the probability of the lady choosing all 6 cups correctly is 0.0010822511 (i.e 1 in 924). As this is a very low probability of the lady simply guessing all 6 cups correctly, we could reject the null hypothesis that she has no special ability to determine which cups of tea had milk added first.   \n",
    "\n",
    "Secondly, the probability of the lady making at most one error (i.e choosing at least 5 cups correctly or choosing all 6 cups correctly), is 0.0400432900. Again, this is a very low probability and again we could reject the null hypothesis.  \n",
    "\n",
    "However, if the we look at the probability of lady making two errors (i.e choosing 4 cups correctly), the probability is 0.2435064935. This is an almost 1 in 4 chance (almost 25%) of her guessing 4 cups correctly. Thus, this probability is much higher and the likelihood of the woman simply guessing 4 cups correctly could be said to be more reasonably possible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 References \n",
    "\n",
    "https://www.acsh.org/news/2017/11/08/how-statistics-came-be-woman-and-cup-tea-12080#:~:text=With%2070%20possible%20combinations%2C%20there%20is%20exactly%20one,high%20to%20distinguish%20lucky%20guessing%20from%20actually%20knowing. \n",
    "\n",
    "https://statisticseasily.com/lady-tasting-tea/\n",
    "\n",
    "https://docs.python.org/3/library/random.html#random.sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "\n",
    "In this task you will assess whether numpy.random.normal() properly generates normal values. To begin, generate a sample of one hundred thousand values using the function with mean 10.0 and standard deviation 3.0.  \n",
    "\n",
    "Use the scipy.stats.shapiro() function to test whether your sample came from a normal distribution. Explain the results and output.  \n",
    "\n",
    "Plot a histogram of your values and plot the corresponding normal distribution probability density function on top of it.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######\n",
    "In order to check if the function numpy.random.normal() correctly generates normal values, I will complete the following:  \n",
    "Firstly, I will generate a sample of one hundred thousand values using the function. The mean will be set at 10.0 and the standard deviation set to 3.0  \n",
    "Then, I will use the scipy.stats.shapiro() function to test if the sample originates from a normal distribution.\n",
    "I will then create a histogram of the generated values and overlay this with the probability density function of the corresponding normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am generating a sample of one hundred thousand values with mean 10.0 and standard deviation 3.0\n",
    "mean = 10.0\n",
    "std_dev = 3.0\n",
    "sample_size = 100000\n",
    "sample_data = np.random.normal(loc=mean, scale=std_dev, size=sample_size)\n",
    "\n",
    "print(\"The sample data is:\", sample_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, as per the task outline, I will use the scipy.stats.shapiro() function to determine if the sample_data comes from a normally distributed dataset. \n",
    "\n",
    "The above function is based on the Shapiro Wilk Test for Normality.  \n",
    "There are two aspects to this test. Firstly, the Shapiro Wilk Test Statistic measures how closely the sample data matches a normal distribution, with a range from 0 to 1. The closer this result is to 1, the higher the likelihood that the data is normally disbuted.\n",
    "The Shapiro Wilk Test also operates on the basis of the null hypothesis that the data sample is drawn from a normally distributed dataset. The p-value is a probability value which helps us to determine whether or not to reject the null hypothesis. The p-value also ranges from 0 to 1.\n",
    "If the p-value is greater than 0.05, this means that we do not reject the null hypothesis, which indicates that the data sample is likely to have a normal distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the Shapiro test on the generated sample\n",
    "shapiro_test = stats.shapiro(sample_data)\n",
    "\n",
    "# Performing the Shapiro-Wilk test\n",
    "print(\"Shapiro-Wilk Test Results are:\")\n",
    "print(f\"Test Statistic: {shapiro_test.statistic:.6f}\") #  this is a measure of how well the sample data fits a normal distribution (to 6 figures after decimanl point)\n",
    "print(f\"p-value: {shapiro_test.pvalue:.6f}\") # if p value > 0.05, indicates the smaple likely comes from a normally distributed dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of the Shapiro-Wilk Test Results\n",
    "\n",
    "The Shapiro Test Statistic result is 0.999966, which is very close to 1. The test statistic ranges from 0 to 1, with values closer to 1 indicating the the sample data aligns with a normal distribution. Thus, we could state that the sample data is very likely to be normally distributed.  \n",
    "\n",
    "In addition, the p value is greater than 0.05, so it means we should not reject the null hypothesis (thus, in this case we can state that the data sample comes from normally distributed dataset). This aligns with what we would expect of the data as we used the numpy.random.normal fucntion to generate the sample data.  *may be an issue with large sizes for p values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I will plot a histogram of your values and plot the corresponding normal distribution probability density function on top of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting a histogram of the generated sample\n",
    "# fig, ax = plt.subplots()\n",
    "\n",
    "'''\n",
    "x = np.linspace(mean - 5*std_dev, mean + 5*std_dev, 100000)\n",
    "#x = np.linspace(-10,30, 100000)\n",
    "pdf = stats.norm.pdf(x, loc=mean, scale=std_dev)\n",
    "plt.plot(x, pdf, 'r', label='Normal Distribution PDF')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the histogram from the generated sample\n",
    "plt.figure(figsize=(7, 5)) # setting the size of the figure\n",
    "plt.hist(sample_data, bins=75, density=True, alpha=0.6, color=\"green\", edgecolor=\"black\", label=\"Sample Data\")\n",
    "\n",
    "# Density curve interval\n",
    "xmin, xmax = plt.xlim() #storing the min and max values of the x-axis to plot the probability density function over the same interval as the histogram.\n",
    "x = np.linspace(xmin, xmax, 1000)\n",
    "p = stats.norm.pdf(x, mean, std_dev)\n",
    "\n",
    "# Combine both the histogram and density curve interval\n",
    "plt.plot(x, p, \"r\", linewidth=2, label=\"Normal Distribution PDF\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.legend()\n",
    "plt.title(\"Sample Data Histogram with Normal Distribution Probability Density Function Overlaid\")\n",
    "plt.show() # show the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Based on the results of the Shapiro-Wilk test, where the test statistic is very close to 1 and the p value is above 0.05, (meaning there is no reason to reject the null hypothesis i.e we can say the data is likely normally distributed), we can say that the data follows a normal distribution.  \n",
    "Furthermore, using the visual aid of the histogram with the probability density function overlaid on it, we can clearly see this is an almost perfect fit and follows the typical 'bell curve' shape associated with normal distribution.  \n",
    "Thus, we can say that numpy.random.normal() generates normally distributed values, based on the mean and standard deviation assigned.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 References\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.shapiro.html#shapiro\n",
    "\n",
    "https://www.geeksforgeeks.org/how-to-perform-a-shapiro-wilk-test-in-python/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "\n",
    "Consider the following dataset containing resting heart rates for patients before and after embarking on a two-week exercise program.\n",
    "\n",
    "| Patient ID |  0 |  1 |  2 |  3 |  4 |  5 |  6 |  7 |  8 |  9 |\n",
    "|:-----------|---:|---:|---:|---:|---:|---:|---:|---:|---:|---:|\n",
    "| Before     | 63 | 68 | 70 | 64 | 74 | 67 | 70 | 57 | 66 | 65 |\n",
    "| After      | 64 | 64 | 68 | 64 | 73 | 70 | 72 | 54 | 61 | 63 |\n",
    "\n",
    "Calculate the t-statistic based on this data set, using Python.\n",
    "Compare it to the value given by `scipy.stats`.\n",
    "Explain your work and list any sources used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this task is to determine whether there is any statistically significant difference in the two sets of data, the resting heart rate before and after an exercise program. We will then look at the difference, if any, that is produced by used the scipy.stats module to calculate the t-statistic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the t-statistic, we will use the t-Test, in particular a paired t-test. This test compares the means of the same group at two different times, in the case the resting heart rate before completing an exercise regime and the resting heart rate after completing the exercise regime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually calculating t statistic\n",
    "Firstly, I will manually calculate the t statistic using numpy. To do this, I will use the following formula for the paired t-test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/2324438/how-to-calculate-the-statistics-t-test-with-numpy\n",
    "# https://machinelearningmastery.com/how-to-code-the-students-t-test-from-scratch-in-python/\n",
    "\n",
    "#Calculation of t statistic using numpy\n",
    "# Creation of the two arrays for t test calculation\n",
    "rhrbefore = np.array([63, 68, 70, 64, 74, 67, 70, 57, 66, 65])\n",
    "rhrafter = np.array([64, 64, 68, 64, 73, 70, 72, 54, 61, 63])\n",
    "\n",
    "difference = rhrbefore - rhrafter #calculates the difference in the heart rates before and after and stores these values\n",
    "\n",
    "meanDiff = np.mean(difference) # mean of the difference of the two different heart rate values per patient\n",
    "stdDiff = np.std(difference) # standard deviation of the difference values\n",
    "\n",
    "tstatistic = meanDiff/(difference.std(ddof=1)/np.sqrt(len(difference))) #ddof set to 1 to include Bessel's correction for working with sample data\n",
    "#standard error of the mean difference\n",
    "#calculated by dividing the standard deviation of the differences and into the sqrt function(calculates the square root of the amount of patients)\n",
    "\n",
    "print(f\"Manually calculated t-statistic is: {tstatistic:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T test using Scipy.stats\n",
    "\n",
    "Next, I will use the scipy.stats formula to caluclate the t statistic. \n",
    "\n",
    "####\n",
    "This is a test for the null hypothesis that two related samples have the same mean. The samples are related in this instance as the values for resting heart rate taken before and after are taken from the same subject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing the paired t-test using scipy stats forumla\n",
    "t_statistic, p_value = stats.ttest_rel(rhrbefore, rhrafter)\n",
    "print(f\"SciPy generated t-statistic: {tstatistic:.5f}\")\n",
    "print(f\"p-value: {p_value:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Results - Manual t statistic calculation versus scipy stats t statistic calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The manual t-statistic calculated using numpy is approximately:\n",
    "Manual t statistic: 1.33723\n",
    "\n",
    "#### The t-statistic calculated using SciPy's stats function is approximately:\n",
    "SciPy t statistic: 1.33723\n",
    "\n",
    "The above two figures are the same, demonstrating that the manual t statistic calculation and the SciPy t statistic calculation both yield the same result and confirming the accuracy of my calculations.   \n",
    "\n",
    "I will now look at the p-value generated by the SciPy t-test to determine if the differnce in the heart rates is statistically significant and to determine if we can accept or reject the null hypothesis.    \n",
    "The null hypothesis in this case is that any mean difference in resting heart rates before and after the exercise program is zero (essentially meaning that there is no difference in heart rate due to the exercise program and any change is due to random chance or other factors).  \n",
    "The p-value in this case is 0.2140. As the p-value > 0.05, this is less than the significance level of 0.05. (The significance level represents a Confidence Interval of 95% - statistically significant).  \n",
    "Thus, there is no evidence here to reject the null hypothesis and so we cannot say that there is any statistically significant difference between the two datasets to confirm the assertion that the exercise regime caused a change in the resting heartbeats of the patients involved.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: ANOVA\n",
    "In this test we will estimate the probability of committing a type II error in specific circumstances. To begin, create a variable called no_type_ii and set it to 0.\n",
    "\n",
    "Now use a loop to perform the following test 10,000 times.\n",
    "\n",
    "Use numpy.random.normal to generate three samples with 100 values each. Give each a standard deviation of 0.1. Give the first sample a mean of 4.9, the second a mean of 5.0, and the third a mean of 5.1.\n",
    "\n",
    "Perform one-way anova on the three samples and add 1 to no_type_ii whenever a type II error occurs.\n",
    "\n",
    "Summarize and explain your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#########\n",
    "This task aims to estimate the probability of committing a type II error in specific circumstances.\n",
    "\n",
    "Here, a type II error is the failure to reject a null hypothesis that is actually false (also called a false negative).\n",
    "\n",
    "To estimate the probability of such errors occuring, I will create a variable called no_type_ii and set it to 0. I will then use a loop to perform the required test 10000 using the specific circumstances set out above in the task.\n",
    "I will then use the stats.f_oneway function to perform one-way ANOVA testing on the three samples created.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize this variable to zero so that it counts the type II errors committed\n",
    "no_type_ii = 0\n",
    "\n",
    "number_trials = 10000\n",
    "\n",
    "# Set the significance level\n",
    "alpha = 0.05\n",
    "\n",
    "# defining a function to generate the samples and perform the anova test\n",
    "def perform_simulation():\n",
    "    # Generate 3 normally distributed random samples with 100 values each, a std deviation of 0.1 and varying means (4.9, 5.0 and 5.1)\n",
    "    sample1 = np.random.normal(loc=4.9, scale=0.1, size=100)\n",
    "    sample2 = np.random.normal(loc=5.0, scale=0.1, size=100)\n",
    "    sample3 = np.random.normal(loc=5.1, scale=0.1, size=100)\n",
    "\n",
    "    # Perform one-way ANOVA test using stats formula\n",
    "    f_statistic, p_value = stats.f_oneway(sample1, sample2, sample3)\n",
    "\n",
    "    # Check for Type II error\n",
    "    return p_value > 0.05\n",
    "\n",
    "# Looping the function to perform the test 10,000 times\n",
    "for _ in range(number_trials):\n",
    "    if perform_simulation():\n",
    "        no_type_ii += 1\n",
    "    \n",
    "\n",
    "# Calculating the probability of committing a Type II error by dividing number type ii errors by thr number of trials\n",
    "prob_type_ii_error = no_type_ii / number_trials\n",
    "\n",
    "print(f\"Number of Type II errors is: {no_type_ii}\")\n",
    "print(f\"Probability of committing a Type II error is: {prob_type_ii_error:.5f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The null hypothesis in this case is that the mean values of the three samples generated are equal. In this task, we are attempting to determine how many times a type ii error occurs, i.e how many times the system fails to reject the null hypothesis that the means are equal.\n",
    "As we have manually set the mean values to differing values, we know that the null hypothesis should be rejected.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fix up headings \n",
    "# put links to each task on top\n",
    "# add references at end and throughout\n",
    "# finish analysis for task 4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
